{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def getInfo(url) :\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    media = ''\n",
    "    \n",
    "    if ( 'foxnews.com' in url ) :\n",
    "        media = 'foxnews'\n",
    "        \n",
    "        date = soup.find('time')['data-time-published']\n",
    "        articleHTML = soup.find('div', attrs={'class':'article-body'})\n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    elif ('cnn.com' in url ) :\n",
    "        media = 'cnn'\n",
    "        date = soup.find('meta', attrs={'itemprop':'dateCreated'})['content']\n",
    "        articleHTML = soup.find('div', attrs={'itemprop':'articleBody'})\n",
    "        \n",
    "        #too random to be more clean\n",
    "        article =articleHTML.text\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif ('breitbart.com' in url ) :\n",
    "        media = 'breitbart'\n",
    "        date = soup.find('span', attrs={'class':'bydate'}).text\n",
    "        articleHTML = soup.find('div', attrs={'class':'entry-content'})\n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "            \n",
    "    elif ('buzzfeed.com' in url ) :\n",
    "        \n",
    "        for div in soup.find_all('div', attrs={'data-module':'action-bar-pagelevel'}): \n",
    "            div.decompose()\n",
    "        \n",
    "        soup.find('span', attrs={'class':'js-subbuzz__title-text'}).decompose()\n",
    "        \n",
    "        media = 'buzzfeed'\n",
    "        date = soup.find('time', attrs={'class':'buzz-timestamp__time js-timestamp__time'}).text\n",
    "        articleHTML = soup.find('article', attrs={'class':'buzz article article--long clearfix'})\n",
    "        \n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "        \n",
    "     \n",
    "    elif ('breitbart.com' in url ) :\n",
    "        media = 'breitbart'\n",
    "        date = soup.find('span', attrs={'class':'bydate'}).text\n",
    "        articleHTML = soup.find('div', attrs={'class':'entry-content'})\n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "            \n",
    "    elif ('20min.ch' in url ) :\n",
    "        media = '20min'\n",
    "        date = soup.find('div', attrs={'class':'published clearfix'}).find('span').text\n",
    "        articleHTML = soup.find('div', attrs={'class':'story_text'})\n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "            \n",
    "    elif ('20min.ch' in url ) :\n",
    "        media = '20min'\n",
    "        date = soup.find('div', attrs={'class':'published clearfix'}).find('span').text\n",
    "        articleHTML = soup.find('div', attrs={'class':'story_text'})\n",
    "        \n",
    "        article = ''\n",
    "        ps = articleHTML.findAll('p')\n",
    "        for p in ps :\n",
    "            article += p.text +'\\n'\n",
    "    \n",
    "        \n",
    "     \n",
    "    hrefs = articleHTML.findAll('a')    \n",
    "    sameHref = []\n",
    "    diffHrefs = []\n",
    "    domainRef=set()\n",
    "    \n",
    "    \n",
    "        \n",
    "    for h in hrefs :\n",
    "        newHref = h.get('href')\n",
    "        if newHref == None or len( newHref.split('/') )== 1 :\n",
    "            continue\n",
    "            \n",
    "        if 'http' in newHref :\n",
    "            domainRef.add(newHref.split('/')[2])\n",
    "        else :\n",
    "            domainRef.add(newHref.split('/')[0])\n",
    "            \n",
    "        if media in  newHref :\n",
    "            sameHref += [newHref]\n",
    "        else :\n",
    "            diffHrefs += [newHref]\n",
    "                    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    return {'sameHref':sameHref,'diffHrefs':diffHrefs, 'domainRef' : domainRef , 'date':date,'article':str(article)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getInfo(\\\n",
    "        'http://www.20min.ch/ro/news/suisse/story/Des-lundi--un-froid-glacial-ressenti-jusqu-a--16-C-29529023'    \\\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
